---
      
title: 1
      
created: 2025-04-30
      
source: Cherry Studio
      
tags: 
      
---

# EE7207 神经网络与深度学习课程学习文档

---

## 第一部分：AI、机器学习、神经网络与深度学习概览

### 1.1 人工智能（AI）与机器学习（ML）
- **人工智能（AI）**：AI是一个广义术语，指使机器能够模仿人类行为的各种技术。
- **机器学习（ML）**：AI的一个子领域，指通过经验（数据）不断改进的算法技术，具有以下特点：
  - 数据驱动：依赖大量数据进行学习。
  - 统计算法：基于统计模型预测或分类。

### 1.2 机器学习、神经网络与深度学习的关系
- **神经网络（NN）**：机器学习的一个子领域，模仿人脑神经元结构，用于处理复杂数据。
- **深度学习（DL）**：神经网络的一个分支，使用多层神经网络结构，从复杂数据中提取特征。

### 1.3 数据科学与机器学习
- **数据科学**：一个涵盖数据处理所有方面的广泛领域，包括数据收集、清洗、分析和建模。
- **机器学习**：专注于从数据中学习的算法，解决如何利用数据进行预测或分类的问题。

### 1.4 浅层与深层神经网络
- **浅层神经网络**：
  - 结构：仅包含1-2层（不包括输入层）。
  - 优点：训练较为简单，计算资源需求较低，所需训练数据较少，适用于某些简单任务。
  - 局限性：对复杂数据（如图像、文本、语音）的特征提取和关系建模能力有限。
- **深层神经网络**：
  - 结构：包含多层隐藏层。
  - 优点：在计算机视觉和自然语言处理等领域表现出色，能够从复杂数据中提取高级特征和关系。
  - 局限性：训练计算成本高，需大量训练数据，容易过拟合。

---

## 第二部分：课程内容目录
1. 神经网络简介
2. 自组织映射（SOM）神经网络
3. 径向基函数（RBF）神经网络
4. 支持向量机（SVM）
5. 多层感知机（MLP）神经网络
6. 卷积神经网络（CNN）与迁移学习
7. 循环与Hopfield神经网络
8. 现代循环神经网络（RNN）
9. 注意力机制与Transformer
10. 自监督学习
11. 图神经网络
12. 深度神经网络应用
13. 高级主题与讨论

---

## 第三部分：神经网络简介

### 3.1 为什么需要人工神经网络？
人类大脑在某些任务（如视觉信息处理）上远超计算机。例如，一岁的婴儿在物体和人脸识别上比许多运行在高速计算机上的AI系统更快、更准确。人类大脑具有以下特点：
- **鲁棒性与容错性**：尽管每天都有神经元死亡，大脑性能几乎不受影响。
- **灵活性**：通过学习适应新环境，无需编程。
- **处理复杂信息**：能够处理模糊、概率性、噪声或不一致的信息。
- **高度并行与非线性**：信息处理方式不同于传统计算机的串行计算。

这些特性是研究神经计算的动机，人工神经网络（ANN）旨在模仿人脑的部分功能。

### 3.2 人工神经网络的生物学起源
- 神经系统由两种细胞组成：神经元（负责信息处理）和胶质细胞（提供支持功能）。
- 生物神经元结构：
  - **细胞体**：信息处理单元。
  - **轴突**：信号传导的主要机制。
  - **树突**：促进轴突信号生成的兴奋和抑制功能。

### 3.3 什么是人工神经网络？
人工神经网络是一种信息处理系统，模仿生物神经网络的部分特性，基于以下假设：
- 信息处理由大量简单元素（称为神经元）完成。
- 信号通过连接链路在神经元之间传递。
- 每个连接链路具有关联权重，用于调整传输信号的强度。
- 每个神经元通过激活函数处理输入信号，决定输出信号。

### 3.4 神经元模型
神经元是神经网络的基本信息处理单元，具有以下三个基本元素：
1. **突触（连接链路）**：每个突触具有权重，输入信号乘以该权重。
2. **加法器**：对输入信号进行加权求和（线性组合）。
3. **激活函数**：对加权和进行非线性处理，并限制输出幅度，通常范围为[0, 1]或[-1, 1]。

数学表示：
$$
u_k = \sum_{j=1}^m w_{kj} x_j
$$
$$
y_k = \phi(u_k + b_k)
$$
其中：
- $x_1, x_2, ..., x_m$ 为输入信号；
- $w_{k1}, w_{k2}, ..., w_{km}$ 为神经元 $k$ 的权重；
- $u_k$ 为加权和（激活信号）；
- $b_k$ 为偏置；
- $\phi(\cdot)$ 为激活函数；
- $y_k$ 为神经元输出。

#### 激活函数类型
1. **二值函数**：
   - 单极二值：输出为0或1。
   - 双极二值：输出为-1或1。
2. **分段线性函数**：在特定范围内线性变化，超出范围则饱和。
3. **Sigmoid函数**：输出范围为(0, 1)，形式为 $\phi(v) = \frac{1}{1 + e^{-v}}$。
4. **双曲正切函数**：输出范围为(-1, 1)，形式为 $\phi(v) = \tanh(v)$。
5. **修正线性单元（ReLU）**：$\phi(v) = \max(0, v)$，在深度学习中广泛使用。

### 3.5 神经网络架构
1. **前馈神经网络**：
   - **单层前馈网络**：仅包含输入层和输出层，输入层不进行计算。
   - **多层前馈网络**：包含一个或多个隐藏层，能够处理复杂的非线性问题。隐藏层输出可作为下一层输入，但并非所有信号都需要传递（如深度网络中的Dropout技术）。
2. **循环神经网络（RNN）**：区别于前馈网络，具有反馈回路，适合处理序列数据（如时间序列、语音）。

### 3.6 神经网络的使用流程
1. **开发阶段**：
   - 选择合适的网络架构。
   - 确定神经元权重，通过训练和测试完成网络开发。
2. **部署阶段**：
   - 将训练好的网络应用到未见过的新数据上。

### 3.7 神经网络的训练与测试
- **训练**：基于已知知识（数据）使用学习算法确定网络权重。
- **知识来源**：
  - 先验信息：已知事实或世界状态。
  - 观测数据：通过传感器获取的样本（有标签或无标签）。
- **有标签数据**：每个输入信号对应一个目标值（标签），如物体识别任务中的“狗”或“猫”标签。
- **无标签数据**：仅有输入信号，无目标值，如一篮水果的图像（无具体水果名称）。
- **数据集划分**：
  - **训练集**：用于训练网络。
  - **测试集**：用于评估网络性能。
  - **验证集**：用于调参（如超参数优化）。

#### 过拟合与欠拟合
- **过拟合**：网络在训练集上表现良好，但在测试集上表现很差，可能是由于过度学习训练数据中的噪声。
- **欠拟合**：网络在训练集上表现不佳，通常也无法在测试集上表现良好，可能是由于训练不足或模型能力不足。

---

## 第四部分：神经网络学习/训练

### 4.1 学习定义
在神经网络中，学习（或训练）是通过环境（数据）的持续刺激，调整网络自由参数（如权重）的过程。学习类型取决于参数更新的方式：
- 网络受环境刺激。
- 网络参数因刺激而更新。
- 更新后，网络以新方式响应。

### 4.2 学习规则
1. **误差校正学习**：
   - 基于误差（目标值与网络输出之差）调整权重。
   - 权重调整公式：$\Delta w_{kj}(n) = \eta e_k(n) x_j(n)$，其中$\eta$为学习率，$e_k(n)$为误差信号。
   - 权重更新与误差信号和输入信号的乘积成正比。
2. **Hebbian学习**：
   - 基于“同时激活”原则：如果连接两侧的神经元同时激活，则连接强度（权重）增加；若不同时激活，则强度减弱或消失。
   - 权重调整公式：$\Delta w_{kj}(n) = \eta x_j(n) y_k(n)$。
   - 权重更新与输入信号和输出信号的乘积成正比。
3. **竞争学习**：
   - 神经元竞争成为“获胜神经元”，获胜者及其邻近神经元更新权重。
   - 权重更新公式：$\Delta w_{kj}(n) = \eta (x_j(n) - w_{kj}(n))$。
   - 权重更新与输入信号和当前权重的差值成正比。

### 4.3 学习范式
1. **监督学习**：
   - 需要“教师”（目标值），通过比较网络输出与期望输出，基于误差调整权重。
   - 类似人类学习语言：听到教师发音，将其存储于记忆，尝试复现并比较误差，多次尝试直至误差较小。
   - **回归与分类**：
     - 回归：将输入映射到连续输出（如预测房价）。
     - 分类：将输入映射到离散输出（如物体识别），可视为寻找决策边界。
   - **线性与非线性分类器**：
     - 线性分类器：仅能处理类间样本可由超平面分隔的问题。
     - 非线性分类器（如神经网络）：可处理类间样本无法由超平面分隔的问题。
   - **分类任务示例**：脑肿瘤检测、物体识别、情感分析、活动识别。
2. **无监督学习**：
   - 不需要“教师”，无目标值，目标是根据数据内在属性进行分组（聚类）。
   - 聚类原则：同一簇内样本相似，不同簇间样本差异较大。
   - **应用示例**：
     - 市场和客户细分（基于需求、位置、兴趣等）。
     - 图像压缩。
     - 医疗保健中的表型聚类。

---

## 第五部分：补充知识与扩展
以下内容基于PPT提及的概念，通过搜索补充背景知识，帮助您更全面地理解课程内容。

### 5.1 激活函数的作用与选择
激活函数在神经网络中引入非线性，使网络能够解决复杂的非线性问题。选择合适的激活函数对网络性能至关重要：
- **Sigmoid**：适用于输出需要概率值的情况（如二分类），但可能导致梯度消失问题。
- **ReLU**：在深度网络中广泛使用，计算简单且能缓解梯度消失问题，但可能导致“死神经元”问题（部分神经元输出恒为0）。
- **Tanh**：输出范围为(-1, 1)，比Sigmoid更容易收敛，但在深层网络中仍可能出现梯度消失。

### 5.2 过拟合与欠拟合的解决方法
- **过拟合解决方案**：
  - 数据增强：通过旋转、翻转等方法扩充训练数据。
  - 正则化：如L2正则化（权重惩罚）或Dropout（随机丢弃部分神经元）。
  - 提前停止：在验证集性能下降时停止训练。
- **欠拟合解决方案**：
  - 增加模型复杂度：如添加更多层或神经元。
  - 延长训练时间：确保模型充分学习。

### 5.3 监督学习与无监督学习的实际应用
- **监督学习**：用于需要明确标签的任务，如垃圾邮件过滤（分类）、房价预测（回归）。
- **无监督学习**：用于探索数据结构，如用户分组（聚类）、异常检测（识别异常数据点）。

---

## 第六部分：课程例题与解答
PPT中提供了两个有标签数据的例子（物体识别和情感分析），以及一个无标签数据的例子（水果篮）。以下是提取的例题及其解答：

### 例题1：物体识别（监督学习）
- **问题**：给定图像，分类为“狗”或“猫”。
- **输入**：图像像素数据。
- **输出**：类别标签（“狗”或“猫”）。
- **解答**：
  1. 数据准备：收集大量标注为“狗”或“猫”的图像数据，划分为训练集、验证集和测试集。
  2. 模型选择：使用卷积神经网络（CNN），因其在图像处理中表现优异。
  3. 训练过程：通过监督学习，基于误差（预测类别与真实标签之差）调整权重，使用交叉熵损失函数。
  4. 测试与评估：在测试集上评估模型准确率，确保泛化能力。

### 例题2：情感分析（监督学习）
- **问题**：给定文本，分类为“正面”、“中立”或“负面”情感。
- **输入**：文本数据。
- **输出**：情感标签。
- **解答**：
  1. 数据预处理：将文本转换为数值表示（如词向量，使用Word2Vec或BERT）。
  2. 模型选择：使用循环神经网络（RNN）或Transformer模型，适合处理序列数据。
  3. 训练过程：通过监督学习，基于误差调整权重，优化分类准确率。
  4. 测试与评估：在测试集上评估模型性能，可能使用F1分数作为指标。

### 例题3：水果篮分组（无监督学习）
- **问题**：给定一篮水果图像（无标签），将其分组。
- **输入**：水果图像数据。
- **输出**：分组结果（簇）。
- **解答**：
  1. 数据预处理：提取图像特征（如颜色、形状、大小）。
  2. 模型选择：使用K-Means聚类算法或自组织映射（SOM）网络。
  3. 训练过程：通过无监督学习，基于特征相似性将水果分为若干簇（如苹果、橙子、香蕉）。
  4. 结果分析：检查簇内样本是否相似，簇间样本是否差异较大，可能需要调整簇数量（如K值）。

---

## 总结与学习建议
本课程涵盖了神经网络与深度学习的基础知识，包括基本概念、模型架构、学习规则及应用示例。建议按照以下步骤学习：
1. 理解基本概念：掌握AI、ML、NN、DL的关系，以及浅层与深层网络的区别。
2. 熟悉神经元模型：重点学习激活函数的作用与数学表达式。
3. 深入学习规则：理解误差校正、Hebbian和竞争学习规则的原理与应用场景。
4. 区分学习范式：明确监督学习与无监督学习的适用任务，结合例题加深理解。
5. 动手实践：使用Python（TensorFlow或PyTorch）实现简单的神经网络模型，解决分类或聚类问题。

如需进一步讨论或补充材料，请随时联系课程团队。

---
