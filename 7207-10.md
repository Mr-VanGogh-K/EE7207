---
      
title: 10
      
created: 2025-04-30
      
source: Cherry Studio
      
tags: 
      
---

### EE7207 课程讲义整理：第10周 - 大语言模型训练与应用

以下是对 EE7207 第10周课程讲义的整理和补充，内容涵盖了 GPT 助理训练流程、基于人类反馈的强化学习（RLHF）、检索增强生成（RAG）、以及大语言模型（LLM）代理系统的相关知识。文档中保留了原始 PPT 的关键细节和推导，并补充了必要的背景知识和示例解答，力求帮助学生全面理解课程内容。

---

## 第一部分：GPT 助理训练流程

### 1.1 训练流程概述
GPT 模型的训练通常包括三个主要阶段：
1. **预训练（Pretraining）**：构建基础模型（Foundation Model），使用大规模互联网文本数据进行训练。
2. **有监督微调（Supervised Finetuning, SFT）**：在高质量标注数据上进一步微调，使模型更贴近特定任务。
3. **基于人类反馈的强化学习（Reinforcement Learning with Human Feedback, RLHF）**：通过奖励模型（Reward Model）和强化学习优化，使模型输出更符合用户期望和社会规范。

以下逐一展开详细说明。

### 1.2 预训练（Pretraining）
- **数据集**：互联网文本，包含数万亿词，低质量但数量庞大。
- **任务**：语言建模，预测下一个单词（Next Word Prediction）。
- **输出**：基础模型（Base Model），如 GPT-3 或 Llama 2。
- **训练资源**：
  - 使用数千块 GPU，训练时间长达数月。
  - 以 GPT-4 为例，训练成本约 1 亿美元。
- **示例模型**：
  - **GPT-3 (2020)**：
    - 词汇量：50,257
    - 上下文长度：2048
    - 参数量：1750 亿
    - 训练数据：3000 亿 token
    - 训练资源：数千块 V100 GPU，数月训练，成本约 500 万美元。
  - **Llama 2 (2023)**：
    - 词汇量：32,000
    - 上下文长度：2048
    - 参数量：650 亿
    - 训练数据：1-1.4 万亿 token
    - 训练资源：2048 块 A100 GPU，21 天训练，成本约 500 万美元。
- **特点**：基础模型可以通过提示（Prompt）完成多种任务，但输出可能不够精准或符合用户期望。

### 1.3 有监督微调（Supervised Finetuning, SFT）
- **数据集**：高质量的问答对或理想助理回复，数量为 10,000-100,000 条。
- **任务**：语言建模，预测下一个单词。
- **输出**：SFT 模型。
- **训练资源**：
  - 基于预训练模型初始化。
  - 使用 1-100 块 GPU，训练时间为数天。
  - 计算量不到预训练的 1%。
- **数据来源**：通常由外包人员准备高质量样本数据。

### 1.4 基于人类反馈的强化学习（RLHF）
#### 1.4.1 目标与流程
- **目标**：使模型输出符合用户期望和社会规范。
- **工作流程**：
  1. 收集人类偏好数据（排名或评分回复）。
  2. 基于偏好数据训练奖励模型（Reward Model, RM），模拟人类偏好。
  3. 使用策略优化（如 PPO，Proximal Policy Optimization）根据奖励模型优化大语言模型。

#### 1.4.2 奖励建模（Reward Modeling, RM）
- **数据集**：人类标注的比较数据，数量为 100,000-1,000,000 条，高质量。
- **任务**：二分类任务，判断哪一个回复更好。
- **输出**：奖励模型（RM）。
- **训练资源**：
  - 基于 SFT 模型初始化。
  - 使用 1-100 块 GPU，训练时间为数天。
- **损失函数**：衡量预测奖励与标注顺序的一致性。

#### 1.4.3 强化学习（Reinforcement Learning, RL）
- **数据集**：高质量提示数据，数量为 10,000-100,000 条。
- **任务**：生成最大化奖励的回复。
- **输出**：RL 模型，如 ChatGPT。
- **训练资源**：
  - 基于 SFT 模型初始化，使用奖励模型指导训练。
  - 使用 1-100 块 GPU，训练时间为数天。

#### 1.4.4 RLHF 的挑战与注意事项
- **反馈偏见**：不同标注者可能有不同观点，导致偏见。
- **奖励黑客（Reward Hacking）**：模型可能利用模式获得高奖励，但未真正提升输出质量。

### 1.5 聊天模型与推理模型的对比
- **聊天模型**：通过有监督微调和 RLHF 优化，擅长对话任务。
- **推理模型**：通过模仿示范解法（SFT）和试错学习（RL），擅长复杂推理任务。
- **示例**：
  - **AlphaGo Zero**：通过自我对弈的强化学习，掌握围棋策略。
  - **DeepseekR1-Zero**：专注于推理任务，通过强化学习优化。

---

## 第二部分：检索增强生成（Retrieval Augmented Generation, RAG）

### 2.1 RAG 概述
- **定义**：RAG 结合了检索（Retrieval）和生成（Generation），通过从文档存储或数据库中提取相关信息，增强语言模型生成内容的准确性和相关性。
- **两阶段流程**：
  1. **检索器（Retriever）**：根据查询，从文档库中识别最相关的文本片段。
  2. **生成器（Generator）**：基于检索到的上下文生成回答。

### 2.2 RAG 组件
RAG 系统涉及以下关键组件和挑战：
- **数据加载**：包括分块（Chunking）、上下文分块、元数据提取。
- **检索**：包括语义搜索、混合搜索、重新排序。
- **生成**：基于检索内容生成回答。
- **后处理**：包括反思、过滤、处理缩写等。
- **挑战**：
  - 上下文外信息、幻觉（Hallucination）、无关内容、偏见。
  - 提示工程、嵌入微调、LLM 微调等技术用于优化。

#### 2.2.1 数据加载 - 分块（Chunking）
- 将大文档分割成小片段（Chunks），便于检索引擎索引和匹配。
- 示例：将文档分成多个块，每块生成嵌入（Embedding），存储于向量数据库（VectorDB）。

#### 2.2.2 数据加载 - 上下文分块
- 在分块时保留上下文信息，如子文档摘要，确保检索时不丢失关键信息。

#### 2.2.3 数据加载 - 元数据提取
- 提取关键词、标题、实体、日期等元数据，用于过滤和优化检索。

#### 2.2.4 检索 - 混合搜索
- 结合语义相似性（Semantic Similarity）和关键词匹配，提升检索准确性。
- 流程：查询嵌入与知识库嵌入匹配，同时考虑关键词。

### 2.3 RAG 评估
#### 2.3.1 评估目标
- 判断 RAG 系统检索相关信息和生成准确、一致输出的能力。
- 识别弱点，指导改进，比较不同方法。

#### 2.3.2 评估挑战
- 检索器和生成器是两个独立但关联的组件。
- 需要同时衡量检索相关性和生成质量。
- 结合自动评估和人工评估方法。

#### 2.3.3 评估两个阶段
1. **检索器评估**：
   - 重点：检索相关文档或段落的有效性。
   - 典型指标：
     - **Recall@k**：前 k 个结果中相关文档的比例。例：查询有 5 个相关文档，前 3 个结果包含 2 个，Recall@3 = 2/5 = 0.4。
     - **Precision@k**：前 k 个结果中相关文档的比例。例：前 5 个结果包含 3 个相关文档，Precision@5 = 3/5 = 0.6。
     - **Mean Reciprocal Rank (MRR)**：强调首个相关结果的位置。例：查询 A 首个相关文档在第 1 位，查询 B 在第 4 位，MRR = (1.0 + 0.25)/2 = 0.625。
     - **Mean Average Precision (MAP)**：综合考虑所有相关文档的排名位置。
2. **生成器评估**：
   - 重点：最终文本输出的准确性、流畅性和相关性。
   - 典型指标：
     - **ROUGE（Recall-Oriented Understudy for Gisting Evaluation）**：衡量生成文本与参考文本的 n-gram 重叠或最长公共子序列（LCS）。例：参考文本“The cat sat on the mat”，生成文本“The cat is on the mat”，ROUGE-1 Recall = 5/6 ≈ 0.833。
     - **BLEU（Bilingual Evaluation Understudy）**：主要用于机器翻译，基于 n-gram 精度和简洁性惩罚。
     - **BERTScore**：基于 BERT 嵌入的语义相似性评估，捕捉词义相似性。
     - **LLM 作为评判者**：利用大语言模型评估生成文本的事实准确性、一致性和完整性。

---

## 第三部分：大语言模型代理系统（LLM Agents）

### 3.1 LLM 代理定义
- **定义**：LLM 代理是一个以大语言模型为核心推理引擎的系统，具备规划、记忆和工具使用等额外能力。
- **关键组件**：
  - **LLM 核心**：用于理解和生成文本。
  - **规划**：制定行动序列的逻辑或策略。
  - **记忆**：存储上下文和历史信息。
  - **工具**：调用外部 API 或模块。

### 3.2 规划与推理
- **规划**：帮助代理分解问题，决定工具使用和顺序。
- **推理技术**：
  - **Chain-of-Thought (CoT)**：通过提示模型逐步推理，提升透明度和准确性。
  - **ReACT (Reason + Act)**：交替推理和行动，动态调整计划。

#### 3.2.1 规划方法
- **顺序规划（Sequential Planning）**：任务按顺序执行，适用于简单任务。
- **层次规划（Hierarchical Planning）**：将复杂目标分解为子目标和原子任务，适用于多层次任务。
- **示例**：撰写研究论文（目标）分解为文献综述、数据收集、分析、草稿撰写等子目标。

#### 3.2.2 ReACT 工作流程
1. 接收用户查询或任务。
2. 推理阶段：形成初步计划或识别信息缺口。
3. 行动阶段：调用工具（如搜索、计算）。
4. 观察反馈：获取工具返回结果。
5. 优化推理：基于新数据更新计划。
6. 重复或输出最终答案。

### 3.3 记忆机制
- **短期记忆**：在当前上下文窗口内存储对话或任务信息。
- **长期记忆**：通过外部存储（如向量数据库）记录历史数据，使用 RAG 机制检索。

### 3.4 工具使用
- **工具定义**：外部服务或 API，用于扩展 LLM 能力。
- **常见工具**：搜索引擎、数据库查询、计算工具、代码执行环境。
- **结果解析与验证**：处理结构化和非结构化输出，进行有效性检查。

### 3.5 代理模式
- **自主代理（Autonomous Agents）**：独立思考和适应，适用于复杂不确定环境。
- **代理工作流（Agentic Workflow）**：基于预定义流程，适用于确定性任务。

---

## 第四部分：示例与解答

### 示例 1：ROUGE 评估
- **参考文本**：The cat sat on the mat
- **生成文本**：The cat is on the mat
- **ROUGE-1**：
  - 匹配单字：5 个（the, the, cat, on, mat）
  - 参考单字总数：6，生成单字总数：6
  - Recall = 5/6 ≈ 0.833，Precision = 5/6 ≈ 0.833，F1 = 0.833
- **ROUGE-2**：
  - 匹配二字组：3 个（the cat, on the, the mat）
  - 参考二字组总数：5，生成二字组总数：5
  - Recall = 3/5 = 0.6，Precision = 3/5 = 0.6，F1 = 0.6
- **ROUGE-L**：
  - 最长公共子序列（LCS）：5 个（the, cat, on, the, mat）
  - Recall = 5/6 ≈ 0.833，Precision = 5/6 ≈ 0.833，F1 = 0.833

### 示例 2：Recall@k 和 Precision@k
- **查询相关文档总数**：5 个
- **前 3 个结果包含相关文档**：2 个
- **Recall@3** = 2/5 = 0.4
- **前 5 个结果包含相关文档**：3 个
- **Precision@5** = 3/5 = 0.6

---

## 第五部分：补充资源与工具
- **RAG 评估工具**：DeepEval, Ragas, TruLens。
- **外部链接**：课程中提供的 Microsoft Build 会议和 YouTube 视频可作为深入学习的资源。
