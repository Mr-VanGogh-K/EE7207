---
      
title: 9
      
created: 2025-04-30
      
source: Cherry Studio
      
tags: 
      
---

# EE7207 第9周课程笔记：编码器-解码器架构与Transformer模型

## 1. 编码器-解码器架构 (Encoder-Decoder Architecture)

### 1.1 基本概念
编码器-解码器架构是序列到序列（Seq2Seq）任务中常用的模型结构，广泛应用于机器翻译、文本生成等任务中。其基本组成如下：
- **编码器 (Encoder)**：读取输入序列，并将其编码为一个固定大小的表示（通常是一组隐藏状态或上下文向量）。
- **解码器 (Decoder)**：基于编码器生成的上下文表示，逐步解码生成目标序列。

**图形表示**：
```
输入序列：x₁ → x₂ → x₃ → ... → xₜ
编码器隐藏状态：h₁ → h₂ → h₃ → ... → hₜ (上下文向量)
解码器输出序列：y₁ → y₂ → y₃ → ...
```

### 1.2 信息瓶颈问题 (Bottleneck Problem)
- **问题描述**：编码器将整个输入序列压缩成一个固定长度的向量 `hₜ`，这可能导致信息丢失，尤其是在输入序列较长时，无法捕捉输入中的所有必要信息。
- **影响**：信息瓶颈限制了模型对长序列的处理能力，导致解码器生成的目标序列质量下降。

## 2. 注意力机制 (Attention Mechanism)

### 2.1 注意力机制的基本思想
为了解决信息瓶颈问题，注意力机制被引入。其核心思想是：**解码器在生成每个输出时，不仅仅依赖编码器的最后一个隐藏状态 `hₜ`，而是动态地关注编码器的所有隐藏状态**。
- **加权求和**：对编码器的所有隐藏状态 `h₁, h₂, ..., hₜ` 进行加权求和，得到上下文向量。
- **权重计算**：权重 `α_i` 表示解码器当前状态 `sₜ` 对编码器第 `i` 个隐藏状态的关注度。

**计算步骤**：
1. 计算评分 (Score)：通过点积或其他方式计算解码器状态 `sₜ` 和每个编码器隐藏状态 `h_i` 的相关性，例如 `score_i = sₜ · h_i`。
2. 归一化权重：使用 softmax 函数将评分转化为概率分布，`α_i = exp(score_i) / Σ exp(score_j)`。
3. 加权求和：计算上下文向量，`context = Σ α_i * h_i`。

### 2.2 基于注意力的LSTM模型
在基于循环神经网络（RNN）或长短期记忆网络（LSTM）的Seq2Seq模型中，注意力机制被广泛应用：
- **编码器**：输入序列经过LSTM处理，生成一系列隐藏状态 `h₁, h₂, ..., hₜ`。
- **解码器**：结合注意力机制，动态计算上下文向量，用于生成目标序列的每个输出 `y_i`。

**注意分布 (Attention Distribution)**：
- 通过 softmax 函数将注意力评分转化为概率分布，指示解码器在当前步骤中对输入序列中每个位置的关注程度。
- 例如，注意力分布可能显示解码器主要关注编码器的最后一个隐藏状态。

### 2.3 键-查询-值注意力 (Key-Query-Value Attention)
- **问题**：点积注意力机制在编码器和解码器隐藏状态维度不同时表现不佳。
- **解决方法**：引入键-查询-值（KQV）注意力机制，灵感来源于信息检索：
  - **查询 (Query)**：解码器当前状态，表示“正在寻找什么”。
  - **键 (Key)**：编码器隐藏状态的索引属性。
  - **值 (Value)**：编码器隐藏状态的实际内容。
- **计算过程**：通过查询和键的匹配计算注意力权重，然后对值进行加权求和，生成上下文向量。

## 3. 超越RNN：Transformer模型的动机

### 3.1 RNN的局限性
- **序列化处理**：RNN按顺序处理输入（逐个token），难以利用现代并行硬件加速。
- **长距离依赖问题**：随着序列长度的增加，RNN难以捕捉远距离token之间的依赖关系。

### 3.2 Transformer模型简介
- **文献参考**：Vaswani et al., "Attention Is All You Need" (2017)，arXiv:1706.03762。
- **核心思想**：完全基于注意力机制，抛弃RNN的序列化处理，支持并行计算。
- **结构**：由多层编码器和解码器组成，每层包含自注意力（Self-Attention）和全连接层（Feed-Forward Neural Network, FFN）。

## 4. Transformer模型详解

### 4.1 编码器 (Encoder)
- **输入**：序列 `x₁, x₂, ..., xₜ`，经过词嵌入和位置编码处理。
- **输出**：上下文相关的隐藏状态 `h₁, h₂, ..., hₜ`。
- **结构**：包含多层块（Block），每层包括：
  - **多头自注意力 (Multi-Head Self-Attention)**：捕获序列中各token之间的关系。
  - **全连接层 (Feed-Forward Neural Network, FFN)**：对每个token进行非线性变换。
  - **残差连接与层归一化 (Residual Connection & Layer Normalization)**：稳定训练并加速收敛。

### 4.2 位置编码 (Positional Encoding)
- **作用**：由于Transformer不具备序列顺序信息，必须通过位置编码为每个token添加位置信息。
- **实现方式**：固定正弦/余弦函数或可学习的参数，参考文献：Machine Learning Mastery 文章。
- **公式**：
  - 对于位置 `pos` 和维度 `i`，编码值为：
    - `PE(pos, 2i) = sin(pos / 10000^(2i/d_model))`
    - `PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))`
  - 其中 `d_model` 是嵌入维度。

### 4.3 自注意力机制 (Self-Attention Mechanism)
- **核心思想**：每个token通过查询（Query, Q）、键（Key, K）和值（Value, V）向量，与序列中其他token交互，捕捉全局上下文。
- **计算步骤**：
  1. 将每个token投影为Q、K、V向量。
  2. 计算注意力分数：`Attention(Q, K, V) = softmax((Q·K^T) / sqrt(d_k)) · V`，其中 `d_k` 是K的维度。
  3. 通过softmax得到注意力权重，对V进行加权求和，更新每个token的表示。
- **多头机制 (Multi-Head Attention)**：将输入分成多个子空间，分别应用自注意力，然后拼接结果，学习不同的token关系。

### 4.4 层归一化 (Layer Normalization)
- **定义**：对每个数据点在所有特征维度上进行归一化，与批归一化（Batch Normalization）不同，后者是对批次中所有数据点在每个特征上归一化。
- **示例**：对数据点Alice（年龄19，身高158），Layer Norm在19和158上进行归一化。

### 4.5 解码器 (Decoder)
- **输入**：目标序列的起始token（例如 `<begin>`）和编码器的输出。
- **输出**：目标序列 `y₁, y₂, ..., yₜ`。
- **结构**：与编码器类似，但增加了：
  - **掩码多头自注意力 (Masked Multi-Head Self-Attention)**：防止解码器在预测时看到未来token。
  - **交叉注意力 (Cross-Attention)**：解码器关注编码器的输出。

### 4.6 Transformer的局限性
- **注意力复杂度**：自注意力计算复杂度为 `O(n²)`，对长序列计算成本高。
- **内存使用**：存储注意力权重矩阵（`n×n`）对长输入耗费大量内存。
- **参数规模**：Transformer模型通常参数量大，需强大GPU/TPU支持。
- **数据需求**：需要大规模数据训练，小数据集容易过拟合。
- **长序列处理**：因计算成本高，长序列常被截断或分割，可能丢失全局上下文。

## 5. 自监督学习 (Self-Supervised Learning)

### 5.1 定义
- 自监督学习是一种从无标签数据中生成训练标签的技术，将无监督问题转化为监督问题。
- **优势**：无需人工标注，允许模型从大规模无标签数据中学习。

### 5.2 动机
- **标注数据成本高**：传统监督学习依赖昂贵且耗时的标注数据。
- **数据准备周期长**：数据清洗、标注和重组过程复杂。

### 5.3 类型
- **生成模型**：学习数据概率分布，生成类似训练数据的新样本，例如GPT。优点是可生成数据，缺点是参数多、计算成本高。
- **判别模型**：学习类别间的决策边界，例如BERT。优点是分类任务效率高，缺点是无法生成新数据。

## 6. BERT (Bidirectional Encoder Representations from Transformers)

### 6.1 基本介绍
- **架构**：仅使用Transformer的编码器部分。
- **型号**：
  - BERT_BASE：12层编码器，12个注意力头，1.1亿参数。
  - BERT_LARGE：24层编码器，16个注意力头，3.4亿参数。
- **训练任务**：
  1. **掩码语言模型 (Masked Language Model, MLM)**：随机掩码句子中15%的词，预测被掩码的词。例如，输入“Paris is the [MASK] city of France”，预测“capital”。
  2. **下一句预测 (Next Sentence Prediction, NSP)**：判断两句话是否连续（50%真实下一句，50%随机句子）。

### 6.2 微调 (Finetuning)
- **过程**：在预训练BERT基础上，添加任务特定层（如分类层），用少量标注数据微调。
- **适用任务**：情感分析、文本分类、机器翻译、命名实体识别（NER）、摘要生成。

### 6.3 微调中的潜在问题
- 仍依赖标注数据。
- 对未直接微调的相关任务表现不佳。

### 6.4 BERT变种
- **RoBERTa**：移除NSP任务，使用更大数据集和动态掩码，性能提升。
- **DistilBERT**：通过知识蒸馏减少模型规模，6层，推理速度提升60%，参数减少40%。
- **ALBERT**：通过嵌入分解和跨层参数共享减少参数量，引入句子顺序预测（SOP）任务。
- **FinBERT**：针对金融领域文本预训练，性能优于通用BERT。

## 7. GPT (Generative Pre-trained Transformer)

### 7.1 基本介绍
- **架构**：仅使用Transformer的解码器部分。
- **型号**：
  - GPT-2：15亿参数，训练数据为WebText。
  - GPT-3：1750亿参数，训练数据包括Common Crawl等。
  - GPT-4：传闻1.76万亿参数。
- **训练任务**：单向自回归语言建模，预测下一个token。

### 7.2 使用方法
- **提示工程 (Prompt Engineering)**：通过详细提示完成特定任务。
- **零样本学习 (Zero-Shot Learning)**：无需训练，直接执行任务。
- **少样本学习 (Few-Shot Learning)**：提供少量示例完成任务，无需更新参数。

### 7.3 适用任务
- 文本生成。
- 对话系统和聊天机器人。
- **权衡**：GPT不考虑右侧未来上下文，擅长生成但在双向理解任务中不如BERT。

## 8. BERT与GPT对比
| 特性             | BERT                           | GPT                           |
|------------------|-------------------------------|-------------------------------|
| 架构             | 仅编码器                      | 仅解码器                      |
| 训练任务         | MLM + NSP                     | 自回归语言建模                |
| 优势             | 理解（双向上下文）            | 生成（单向上下文）            |
| 适用任务         | 分类、NER、情感分析等         | 文本生成、对话系统等          |

## 9. 微调技术与优化策略

### 9.1 渐进解冻 (Gradual Unfreezing)
- **定义**：初始冻结预训练层，仅训练新分类头，逐步解冻更深层进行微调。
- **优点**：避免灾难性遗忘，稳定训练，提升泛化能力。

### 9.2 判别性微调 (Discriminative Fine-Tuning)
- **定义**：不同层组使用不同学习率，顶层和分类头学习率较高，底层较低。
- **参考**：Howard et al., "Universal Language Model Fine-tuning for Text Classification"。

### 9.3 学习率调度 (Learning Rate Scheduling)
- **方法**：
  - 常量学习率：简单但适应性差。
  - 步长衰减：按固定步长减少学习率。
  - 指数衰减：平滑减少学习率。
  - 周期性调度：学习率在范围内周期波动。
  - 预热+衰减：初始学习率逐渐上升后衰减，适合Transformer训练。

### 9.4 学习率查找 (Learning Rate Finder)
- **方法**：从极低学习率开始，逐步增加，记录损失，选择损失急剧下降前的学习率范围。

### 9.5 使用LLM为BERT生成微调标签
- **方法**：利用大语言模型（LLM）为无标签数据生成伪标签，用于BERT微调。
- **优点**：快速生成大规模标注数据，成本低。
- **注意事项**：需验证伪标签质量，避免偏差或幻觉。

## 10. 总结
本课程介绍了编码器-解码器架构及其瓶颈问题，注意力机制的引入和改进（如KQV注意力），以及Transformer模型的结构与优势。同时，探讨了自监督学习的重要性，以及BERT和GPT作为Transformer变种在理解和生成任务中的应用。最后，介绍了微调技术和优化策略，帮助模型适应特定任务。
