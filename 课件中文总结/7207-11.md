---
      
title: 11
      
created: 2025-04-30
      
source: Cherry Studio
      
tags: 
      
---

# EE7207 第11讲：图神经网络（Graph Neural Networks, GNN）

---


## 1. 图的表示（Graph Representation）

### 1.1 图的基本定义
- **图（Graph）**：图是一种表示实体和它们之间关系的数学结构，定义为$G = (V, E)$，其中：
  -$V = \{v_1, ..., v_N\}$表示节点（Nodes）或顶点（Vertices），代表实体。
  -$E = \{e_1, ..., e_M\}$表示边（Edges）或链接（Links），代表实体之间的交互或关系。
- **节点特征（Node Features）**：每个节点可能关联一组特征向量$X = \{x_1, ..., x_N\}$，用于描述节点的属性。例如，一个社交网络中，节点特征可能包括用户的年龄、性别、职业等。
- **示例**：在一个社交网络中，节点可能是人，边表示友谊关系，节点特征可能是个人资料信息。

### 1.2 图的类型
- **有向图与无向图（Directed vs. Undirected Graphs）**：
  - 有向图：边的关系是有方向的（有序对），如Twitter上的“关注”关系。
  - 无向图：边的关系是双向的，如Facebook上的“好友”关系。
- **加权图与非加权图（Weighted vs. Unweighted Graphs）**：
  - 加权图：边带有权重，表示关系的强度或距离（如两个城市间的距离）。
  - 非加权图：边仅表示连接关系，无权重。
- **简单图与多重图（Simple vs. Multi-graphs）**：
  - 简单图：任意两个节点之间最多只有一条边。
  - 多重图：允许两个节点之间有多条边。
- **同构图与异构图（Homogeneous vs. Heterogeneous Graphs）**：
  - 同构图：所有节点和边的类型相同。
  - 异构图：节点和边具有不同类型（如知识图谱中包含多种实体和关系）。

### 1.3 图的表示方法
图可以用多种数据结构表示，每种方法有其优缺点：
- **邻接矩阵（Adjacency Matrix）**：
  - 定义：一个$|V| \times |V|$的矩阵$A$，其中$A_{ij} = 1$表示节点$i$到节点$j$有一条边，否则$A_{ij} = 0$。
  - 特点：对于无向图，邻接矩阵是对称的；对于稀疏图，矩阵中大部分元素为0，存储效率低。
- **边列表（Edge List）**：
  - 定义：记录所有边的列表，每条边表示为一个节点对$(i, j)$。
  - 特点：存储空间小，但查询特定节点邻居效率低。
- **邻接列表（Adjacency List）**：
  - 定义：为每个节点存储一个邻居列表。
  - 特点：适合稀疏图，查询邻居效率高。
- **比较**：
  - 邻接矩阵适合密集图，操作（如矩阵运算）方便，但存储成本高。
  - 边列表和邻接列表适合稀疏图，存储效率高，但部分操作（如检查边是否存在）较慢。

### 1.4 图数据的挑战
- 图数据的复杂性：图数据通常不具有规则的网格结构（如图像），节点顺序不固定，图的大小和拓扑结构任意，导致传统神经网络（如CNN）无法直接应用。
- 示例：一个社交网络图中，节点的连接关系（邻居）数量不一，无法像图像像素那样用固定大小的滤波器处理。

---

## 2. 机器学习类型与图数据（Machine Learning Types with Graph Data）

### 2.1 图数据的广泛应用
图数据在现实世界中无处不在，典型应用包括：
- **医学/制药**：分子结构图，用于药物发现。
- **社交网络**：用户关系图，用于推荐系统。
- **知识图谱**：实体和关系图，用于问答系统。
- **其他**：3D游戏中的网格、地图导航、犯罪证据关联图、引用网络、代码依赖图等。

### 2.2 图数据上的机器学习问题
根据预测对象不同，图数据的机器学习任务可分为以下几类：
- **节点级预测（Node-level Predictions）**：
  - 任务：预测单个节点的属性或标签。
  - 示例：预测社交网络中某个用户是否吸烟。
- **边级预测（Edge-level Predictions）**：
  - 任务：预测两个节点之间是否存在链接（链接预测）。
  - 示例：Facebook推荐“你可能认识的人”。
- **图级预测（Graph-level Predictions）**：
  - 任务：预测整个图的属性。
  - 示例：预测某个分子是否适合作为药物。
- **子图预测（Subgraph Predictions）**：
  - 任务：检测节点是否形成社区（如社交圈检测）。
- **图生成（Graph Generation）**：
  - 任务：生成具有特定性质的新图。
  - 示例：药物发现中生成新分子结构。
- **图演化（Graph Evolution）**：
  - 任务：模拟图随时间变化的动态过程。
  - 示例：物理仿真中模拟复杂系统的演化。

### 2.3 图表示学习（Graph Representation Learning）
- **目标**：将图中的节点映射到低维嵌入空间（d维向量），使得网络中相似的节点在嵌入空间中更靠近。
- **方法**：
  - 浅层编码（Shallow Encoding）：直接查找嵌入表，每个节点有独立嵌入。
    - 缺点：参数量与节点数成正比（$O(|V|)$），无法泛化到未见节点（属于“转导学习”）。
  - 深层编码（Deep Graph Encoders）：通过多层神经网络，利用图结构学习节点嵌入。
- **转导学习与归纳学习**：
  - **转导学习（Transductive Learning）**：在部分标记的图上训练，测试时预测同一图内未标记节点的属性，无法处理新节点。
  - **归纳学习（Inductive Learning）**：训练一个通用的映射函数，可泛化到未见节点或新图。

---

## 3. 图神经网络基础（Graph Neural Networks: Basics）

### 3.1 图数据与传统数据的差异
- 图数据与图像、语音、文本等传统数据相比，具有以下特点：
  - 大小任意：图的节点数和边数不固定。
  - 拓扑结构复杂：无固定邻域或滑动窗口概念。
  - 节点顺序不固定：图是排列无关的（Permutation Invariant）。
  - 动态性与多模态特征：图结构可能随时间变化，节点可能具有多种类型的特征。

### 3.2 排列不变性与排列等变性
- **排列不变性（Permutation Invariance）**：
  - 定义：图级函数$f(A, X)$应满足$f(A, X) = f(P A P^T, P X)$，即输入图的节点顺序改变后，输出结果不变。
  - 意义：确保图的嵌入不依赖于节点的编号顺序。
- **排列等变性（Permutation Equivariance）**：
  - 定义：节点级函数$f(A, X)$应满足$P f(A, X) = f(P A P^T, P X)$，即输入节点顺序改变后，输出节点的嵌入顺序相应改变。
  - 意义：确保节点嵌入保持图结构中的相对关系。

### 3.3 图神经网络的核心思想
- **邻域聚合（Aggregate Neighbours）**：
  - 基本思想：通过聚合节点的局部邻域信息生成节点嵌入。
  - 方法：利用神经网络，基于节点及其邻居的特征计算嵌入。
  - 多层结构：通过多层GNN，节点嵌入可获取更远邻居（k-hop）的信息。
- **消息传递（Message Passing）**：
  - GNN通过消息传递机制实现信息聚合，每一层包括以下步骤：
    1. **消息计算**：计算邻居节点的信息。
    2. **聚合**：汇总邻居的消息。
    3. **更新**：结合自身信息和聚合信息，更新节点嵌入。
    4. **读出（Readout）**：对于图级任务，聚合所有节点嵌入生成图嵌入。

### 3.4 GNN单层结构
- **公式表示**：
  - 节点$v$在第$l$层的嵌入更新为：
   $$
    h_v^{(l)} = \text{CONCAT} \left( \text{AGG} \left( m_u^{(l)}, u \in N(v) \right), m_v^{(l)} \right)
   $$
    其中$m_u^{(l)}$是邻居节点的消息，$N(v)$是节点$v$的邻域。
- **聚合操作**：常见聚合操作包括求和（Sum）、平均（Mean）、最大值（Max）等。

---

## 4. 图神经网络变体（GNN Variants）

### 4.1 图卷积网络（Graph Convolutional Networks, GCN）
- **核心思想**：基于谱图理论，通过图的拉普拉斯矩阵实现邻域特征的加权聚合。
- **公式**：
 $$
  H^{(l+1)} = \sigma \left( \tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} H^{(l)} W^{(l)} \right)
 $$
  其中：
  -$\tilde{A} = A + I$是加了自环的邻接矩阵。
  -$\tilde{D}$是$\tilde{A}$的度矩阵。
  -$H^{(l)}$是第$l$层的节点特征矩阵。
  -$W^{(l)}$是第$l$层的可学习权重矩阵。
  -$\sigma$是激活函数（如ReLU）。
- **归一化作用**：通过$\tilde{D}^{-\frac{1}{2}}$进行对称归一化，避免高度节点（高连接度）对特征聚合的主导影响。
- **缺点**：
  1. 所有邻居同等重要，缺乏灵活性。
  2. 全局计算导致扩展性差，不适合大图。
  3. 多层堆叠导致过平滑问题（节点嵌入趋于一致）。

### 4.2 图注意力网络（Graph Attention Networks, GAT）
- **核心思想**：通过注意力机制，为不同邻居分配不同的重要性权重。
- **注意力权重计算**：
 $$
  e_{vu} = a(W h_v, W h_u), \quad \alpha_{vu} = \text{softmax}_v(e_{vu})
 $$
  其中$a$是注意力函数，$\alpha_{vu}$是节点$u$对节点$v$的注意力权重。
- **特征更新**：
 $$
  h_v' = \sigma \left( \sum_{u \in N(v)} \alpha_{vu} W h_u \right)
 $$
- **多头注意力（Multi-Head Attention）**：使用多个注意力头，增强表达能力。
- **优点**：能够聚焦重要邻居，适合复杂图结构。

### 4.3 GraphSAGE
- **核心思想**：通过采样和聚合（Sample and Aggregate）实现可扩展性和归纳学习。
- **步骤**：
  1. **节点采样**：从邻域中随机采样固定数量的邻居，降低计算成本。
  2. **特征聚合**：对采样邻居的特征进行聚合（如均值、池化、LSTM）。
  3. **更新**：将自身嵌入与聚合信息拼接，更新节点嵌入。
  4. **归一化**：使用L2归一化确保嵌入向量长度一致。
- **聚合方法**：
  - 均值（Mean）：计算邻居特征的加权平均。
  - 池化（Pool）：对邻居特征应用MLP后取均值或最大值。
  - LSTM：对邻居特征递归应用LSTM（需训练时随机打乱顺序以保持排列不变性）。
- **优点**：
  - 可扩展性：适合大规模图。
  - 归纳能力：能泛化到未见节点。

---

## 5. 图神经网络层实践（GNN Layers in Practice）

### 5.1 GNN层设计
- **现代深度学习模块**：
  - **批归一化（Batch Normalization）**：稳定训练过程，重新中心化和缩放嵌入。
  - **Dropout**：防止过拟合。
  - **注意力或门控机制**：控制消息重要性。
- **过平滑问题**：
  - 问题：多层GNN会导致节点嵌入趋于一致（Over-Smoothing），因为感受野（Receptive Field）随着层数增加而快速扩大。
  - 解决方案：
    1. 设计浅层但强大的GNN，在每层内增加表达能力（如使用深层MLP进行消息传递）。
    2. 添加非GNN层（如MLP预处理和后处理层）。
    3. 使用跳跃连接（Skip Connections），增强早期层的影响。

### 5.2 图特征操作
- **无特征图的处理**：
  - 解决方案1：为节点分配常数值。
  - 解决方案2：为节点分配唯一ID（独热向量）。
- **特征增强**：通过拼接或投影方法增强特征表示。

### 5.3 图结构操作
- **稀疏图增强**：
  - 添加虚拟边：增加连接性。
  - 添加虚拟节点：提高消息传递效率。
- **密集/大规模图处理**：
  - 随机采样邻居：降低计算成本，保持准确性。

### 5.4 下游任务中的GCN
- **示例**：点云语义分割任务，使用GCN骨干网络、融合模块和MLP预测模块实现点级标签预测。

---

## 6. 知识图谱（Knowledge Graph）

### 6.1 异构图（Heterogeneous Graphs）
- **定义**：异构图包含多种类型的节点和边，形式化为$G = (V, E, \tau, \phi)$，其中$\tau(v)$表示节点类型，$\phi(u,v)$表示边类型。
- **关系类型**：边表示为三元组（头节点，关系，尾节点）。
- **应用场景**：
  - 不同类型节点/边具有不同特征形状。
  - 不同关系类型需要不同模型处理。
- **关系图卷积网络（Relational GCN）**：为不同关系类型设计不同的神经网络权重。

### 6.2 知识图谱定义
- **知识图谱**：一种异构图，捕获实体、类型和关系。
  - 节点：实体，带有类型标签。
  - 边：实体之间的关系。
- **挑战**：
  - 规模庞大：包含数百万节点和边。
  - 不完整性：许多真实边缺失。
- **知识图谱补全任务（KG Completion）**：给定（头节点，关系），预测缺失的尾节点。

### 6.3 TransE
- **核心思想**：在嵌入空间中建模实体和关系，目标是满足$h + r \approx t$（当三元组存在时）。
- **评分函数**：$f(h, t) = -||h + r - t||$，最小化真实三元组的距离。
- **缺点**：无法很好处理复杂关系模式（如对称性、反对称性）。

### 6.4 关系模式（Relation Patterns）
- 知识图谱中常见的关系模式包括对称性、反对称性、传递性等，TransE难以直接建模这些模式。

### 6.5 TransR
- **核心思想**：为每个关系设计特定的嵌入空间，通过投影矩阵$M_r$将实体从实体空间投影到关系空间。
- **评分函数**：$f(h, t) = -||h_\perp + r - t_\perp||$，其中$h_\perp = M_r h, t_\perp = M_r t$。
- **优点**：能更好地处理复杂关系模式。

---

## 补充材料：GNN与CNN的对比
- **CNN**：可视为一种特殊的GNN，具有固定邻域大小和顺序。
  - 滤波器大小预定义，输入需统一大小（通过调整或填充）。
  - 缺点：不具有排列不变性，改变像素顺序会导致输出不同。
- **GNN**：处理任意图结构，节点度数不同，具有排列不变性/等变性。

---
