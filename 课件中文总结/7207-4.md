
---

### 支持向量机（SVM）学习文档

以下是对《EE7207-NN4_2025》课程PPT内容的整理、翻译与扩展，旨在帮助你全面理解支持向量机（SVM）的原理、推导、应用及相关例题解答。本文档涵盖了线性SVM、核SVM的基本概念、数学推导、优化问题及应用场景，同时补充了必要的背景知识，并对PPT中提到的例题进行提取和解答。

---

## 1. 支持向量机（SVM）简介

支持向量机（Support Vector Machine, SVM）是一种前馈神经网络，与径向基函数（RBF）神经网络类似，可用于模式分类和回归任务。SVM通过寻找一个最优的决策面（超平面）来分离不同类别的数据，其核心目标是最大化分类边距（margin），从而提高分类器的泛化能力。

核支持向量机（Kernel SVM）是线性SVM的扩展，与RBF神经网络在架构上有相似之处，但其学习原理有显著不同。本章将首先介绍线性SVM的原理，然后扩展到核SVM。

---

## 2. 线性支持向量机（Linear SVM）

### 2.1 基本动机

线性SVM的目标是找到一个超平面，将属于不同类别的训练样本分离开来。假设我们有两类数据，类别标签分别为+1和-1，超平面的数学表达式为：

$$
\mathbf{w}^T \mathbf{x} + b = 0
$$

其中，$\mathbf{x}$ 是特征向量，$\mathbf{w}$ 是权重向量，$b$ 是偏置项。

#### 分类目标
- 对于类别标签为+1的样本，要求 $\mathbf{w}^T \mathbf{x} + b > 0$
- 对于类别标签为-1的样本，要求 $\mathbf{w}^T \mathbf{x} + b < 0$

#### 决策边界与分类置信度
在分类过程中，样本到超平面的距离反映了分类的置信度：
- 如果样本距离超平面较远（如点A），我们对分类结果有较高的置信度。
- 如果样本距离超平面很近（如点C），分类置信度较低，稍有变动可能会导致分类错误。

因此，理想的超平面不仅要正确分类所有样本，还要使样本到超平面的距离尽可能远，即最大化分类边距（margin of separation）。

#### 边距最大化的意义
如下图所示，超平面(b)比超平面(a)更优，因为(b)使样本到超平面的距离更大，分类边距更大。

---

### 2.2 问题建模

#### 分类边距（Margin of Separation）
分类边距是指超平面与最近的数据点之间的距离。SVM的目标是找到一个超平面，使这个边距最大化，此时的决策面称为**最优超平面（optimal hyperplane）**。

#### 距离计算
样本 $\mathbf{x}$ 到超平面的距离可以通过以下公式计算：
$$
r = \frac{|\mathbf{w}^T \mathbf{x} + b|}{\|\mathbf{w}\|}
$$
其中，$\|\mathbf{w}\|$ 是权重向量的欧几里得范数。

为了简化计算，我们可以对 $\mathbf{w}$ 和 $b$ 进行缩放，使得最近的样本点满足：
- 对于类别+1的样本：$\mathbf{w}^T \mathbf{x} + b \geq 1$
- 对于类别-1的样本：$\mathbf{w}^T \mathbf{x} + b \leq -1$

此时，边距 $\rho$ 为：
$$
\rho = \frac{2}{\|\mathbf{w}\|}
$$

#### 支持向量（Support Vectors）
满足等式条件的样本点（即 $\mathbf{w}^T \mathbf{x} + b = \pm 1$）称为支持向量（support vectors）。支持向量是距离超平面最近的点，对最优超平面的确定起关键作用。

---

### 2.3 优化问题

#### 原问题（Primal Problem）
SVM的目标是最大化边距 $\rho = \frac{2}{\|\mathbf{w}\|}$，这等价于最小化 $\|\mathbf{w}\|^2$。因此，优化问题可形式化为：
$$
\min J(\mathbf{w}) = \frac{1}{2} \|\mathbf{w}\|^2
$$
约束条件：
$$
y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1, \quad i = 1, 2, \dots, N
$$
其中，$y_i$ 是样本 $\mathbf{x}_i$ 的类别标签（+1 或 -1）。

#### 拉格朗日乘子法
为了求解上述约束优化问题，引入拉格朗日乘子 $\alpha_i \geq 0$，构造拉格朗日函数：
$$
L(\mathbf{w}, b, \alpha) = \frac{1}{2} \|\mathbf{w}\|^2 - \sum_{i=1}^N \alpha_i [y_i (\mathbf{w}^T \mathbf{x}_i + b) - 1]
$$
通过对 $\mathbf{w}$ 和 $b$ 求偏导并设为0，得到最优性条件：
1. $\mathbf{w} = \sum_{i=1}^N \alpha_i y_i \mathbf{x}_i$
2. $\sum_{i=1}^N \alpha_i y_i = 0$

#### 对偶问题（Dual Problem）
将上述条件代入拉格朗日函数，得到对偶问题：
$$
\max Q(\alpha) = \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j (\mathbf{x}_i^T \mathbf{x}_j)
$$
约束条件：
- $\alpha_i \geq 0, \quad i = 1, 2, \dots, N$
- $\sum_{i=1}^N \alpha_i y_i = 0$

对偶问题是一个二次规划（Quadratic Programming, QP）问题，可以通过QP软件求解 $\alpha_i$。大多数 $\alpha_i$ 为0，只有支持向量对应的 $\alpha_i$ 不为0。

#### 偏置项 $b$ 的计算
利用支持向量计算偏置项：
$$
b = y_k - \mathbf{w}^T \mathbf{x}_k
$$
其中，$\mathbf{x}_k$ 是一个支持向量。为了提高精度，通常对所有支持向量计算 $b$ 并取平均值：
$$
b = \frac{1}{N_s} \sum_{k \in S} (y_k - \mathbf{w}^T \mathbf{x}_k)
$$
其中，$N_s$ 是支持向量的总数，$S$ 是支持向量集合。

---

### 2.4 线性不可分情况下的SVM

当数据线性不可分时，无法找到一个超平面完全正确地分离两类数据。此时，引入**软边距（soft margin）**的概念，允许一定的分类误差。

#### 松弛变量（Slack Variables）
为每个样本 $\mathbf{x}_i$ 引入一个非负松弛变量 $\xi_i \geq 0$，表示样本偏离理想分类条件的程度：
- 如果 $0 < \xi_i < 1$，样本落在分离区域内，但仍在正确一侧。
- 如果 $\xi_i \geq 1$，样本落在错误一侧。

约束条件变为：
$$
y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0
$$

#### 原问题（Primal Problem）
优化目标是平衡边距最大化和分类误差最小化：
$$
\min J(\mathbf{w}, \xi) = \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^N \xi_i
$$
其中，$C$ 是一个超参数，控制对分类误差的惩罚程度。

#### 对偶问题（Dual Problem）
对偶问题形式为：
$$
\max Q(\alpha) = \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j (\mathbf{x}_i^T \mathbf{x}_j)
$$
约束条件：
- $0 \leq \alpha_i \leq C, \quad i = 1, 2, \dots, N$
- $\sum_{i=1}^N \alpha_i y_i = 0$

偏置项 $b$ 的计算与线性可分情况类似，利用支持向量计算。

---

## 3. 核支持向量机（Kernel SVM）

### 3.1 基本思想

当数据在原始输入空间中线性不可分时，可以通过非线性映射将数据投影到一个高维特征空间，在高维空间中寻找线性分离超平面。核SVM的关键步骤是：
1. **非线性映射**：将输入向量 $\mathbf{x}$ 映射到高维特征空间 $\phi(\mathbf{x})$。
2. **线性分离**：在特征空间中构建最优超平面。

#### Cover定理
Cover定理指出：通过非线性变换将非线性不可分的数据映射到足够高维的特征空间后，数据有很大概率变为线性可分。

---

### 3.2 核函数（Kernel Function）

核函数 $K(\mathbf{x}, \mathbf{x}')$ 用于计算特征空间中的内积，而无需显式计算特征向量 $\phi(\mathbf{x})$：
$$
K(\mathbf{x}, \mathbf{x}') = \phi(\mathbf{x})^T \phi(\mathbf{x}')
$$

核函数必须满足Mercer定理，即核矩阵 $K$ 是对称的且半正定的。常用的核函数包括：
- **多项式核**：$K(\mathbf{x}, \mathbf{x}') = (1 + \mathbf{x}^T \mathbf{x}')^p$，其中 $p$ 通常为2或3。
- **高斯核（RBF核）**：$K(\mathbf{x}, \mathbf{x}') = \exp(-\|\mathbf{x} - \mathbf{x}'\|^2 / 2\sigma^2)$，其中 $\sigma$ 是高斯核的宽度。

核函数的优势在于计算复杂度低，避免了直接计算高维特征空间的内积。

---

### 3.3 核SVM的对偶问题

核SVM的对偶问题与线性SVM的对偶问题形式相同，仅将内积 $\mathbf{x}_i^T \mathbf{x}_j$ 替换为核函数 $K(\mathbf{x}_i, \mathbf{x}_j)$：
$$
\max Q(\alpha) = \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j K(\mathbf{x}_i, \mathbf{x}_j)
$$
约束条件：
- $0 \leq \alpha_i \leq C, \quad i = 1, 2, \dots, N$
- $\sum_{i=1}^N \alpha_i y_i = 0$

决策函数为：
$$
f(\mathbf{x}) = \sum_{i \in S} \alpha_i y_i K(\mathbf{x}_i, \mathbf{x}) + b
$$
其中，$S$ 是支持向量集合。

---

### 3.4 核SVM与RBF神经网络的比较

- **架构相似性**：核SVM与RBF神经网络在结构上类似。
- **差异**：在高斯核SVM中，神经元数量和中心向量（支持向量）是自动确定的，而RBF神经网络通常需要通过启发式方法设计。

---

## 4. 例题与解答

以下是PPT中提取的例题及其解答，旨在帮助理解SVM的实际应用。

### 例题1：线性可分数据的SVM分类
**问题描述**：给定一组线性可分的数据，类别标签为+1和-1，求最优超平面。

**解答**：
1. 确定支持向量：支持向量是距离超平面最近的点，满足 $\mathbf{w}^T \mathbf{x} + b = \pm 1$。通过观察数据点，找出最接近决策边界的点。
2. 构建优化问题：使用对偶形式求解拉格朗日乘子 $\alpha_i$。
3. 计算权重和偏置：利用支持向量计算 $\mathbf{w}$ 和 $b$。

（具体数据和计算过程需根据PPT中的图形或数据补充，此处为框架性解答。）

### 例题2：线性不可分数据的软边距SVM
**问题描述**：给定一组线性不可分的数据，求软边距SVM的最优超平面。

**解答**：
1. 引入松弛变量 $\xi_i$，并选择合适的惩罚参数 $C$。
2. 构建对偶问题，使用QP方法求解 $\alpha_i$。注意约束条件为 $0 \leq \alpha_i \leq C$。
3. 确定支持向量和偏置 $b$，构建决策函数。

（同样，具体计算需结合PPT中的数据。）

### 例题3：核SVM应用（高斯核与多项式核）
**问题描述**：给定非线性可分数据，使用高斯核和多项式核构建SVM分类器。

**解答**：
1. 选择核函数：高斯核 $K(\mathbf{x}, \mathbf{x}') = \exp(-\|\mathbf{x} - \mathbf{x}'\|^2 / 2\sigma^2)$ 或多项式核 $K(\mathbf{x}, \mathbf{x}') = (1 + \mathbf{x}^T \mathbf{x}')^p$。
2. 计算核矩阵 $K$，求解对偶问题，得到支持向量和拉格朗日乘子。
3. 构建决策函数，利用核函数计算新样本的分类结果。

（具体参数如 $\sigma$ 和 $p$ 可根据PPT中提供的图形效果选择。）

---

## 5. 总结与补充知识

- **线性SVM**：适用于线性可分数据，通过最大化边距寻找最优超平面，支持向量起关键作用。
- **软边距SVM**：处理线性不可分数据，通过引入松弛变量和惩罚参数 $C$ 平衡边距与分类误差。
- **核SVM**：通过核函数将数据映射到高维空间，解决非线性问题，常用核函数包括高斯核和多项式核。
- **关键推导**：SVM的优化问题从原问题到对偶问题的转换，利用拉格朗日乘子和KKT条件求解，支持向量的确定及决策函数的构建。

### 补充背景知识
- **二次规划（QP）**：SVM的对偶问题是典型的QP问题，可使用现成的优化工具（如SMO算法）求解。
- **Mercer定理**：核函数必须满足对称性和半正定性，保证特征空间的有效性。
- **参数选择**：$C$ 和核函数参数（如高斯核的 $\sigma$）通常通过交叉验证确定。

