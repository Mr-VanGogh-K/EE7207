---
      
title: 8
      
created: 2025-04-30
      
source: Cherry Studio
      
tags: 
      
---

## 第一部分：神经网络基础
---

### 1.2 神经元（Neuron）的基础
神经元是神经网络的基本单元，执行以下操作：
1. **输入处理**：
   - 接收输入数据 $(x_1, x_2, ..., x_n)$。
   - 每个输入与一个权重 $(w_1, w_2, ..., w_n)$ 相关联，权重决定输入的重要性。
2. **计算加权和**：
   - 计算输入的加权和：$z = \sum_{i=1}^{n} w_i x_i + b$，其中$b$ 是偏置（bias），用于调整激活函数的偏移，帮助模型更好地学习。
3. **应用激活函数**：
   - 将加权和$z$通过激活函数处理，引入非线性。常用激活函数包括：
     - **Sigmoid**：$f(z) = \frac{1}{1 + e^{-z}}$，输出范围为 (0, 1)。
     - **ReLU（Rectified Linear Unit）**：$f(z) = \max(0, z)$，输出非负值，计算简单且有效。
4. **输出结果**：
   - 激活函数的输出作为下一层神经元的输入，或者作为网络的最终输出。

#### **为何需要非线性激活函数？**
- 如果不使用非线性激活函数，神经网络的所有层（输入层、隐藏层、输出层）本质上会退化为一个单层线性模型，因为线性函数的组合仍然是线性函数。
- 根据**通用逼近定理（Universal Approximation Theorem）**，具有至少一个隐藏层、使用非线性激活函数且神经元数量足够的神经网络可以逼近任何连续函数。

---

### 1.3 神经网络结构
神经网络由多层相互连接的神经元组成，典型结构包括以下三类层：
- **输入层（Input Layer）**：接收原始数据作为网络输入。
- **隐藏层（Hidden Layers）**：执行计算，提取输入数据的模式和特征，负责学习数据中的复杂关系。
- **输出层（Output Layer）**：生成网络的最终计算结果。

---

### 1.4 前向传播（Forward Propagation）与损失（Loss）
- **前向传播**：神经网络接收输入数据$x$，通过各层计算，生成预测输出$\hat{y}$。
- **损失（Loss）**：衡量预测输出$\hat{y}$与实际输出$y$之间的差异，用于评估模型性能。
  - 损失越小，模型预测越接近真实值。
  - 损失函数为学习过程提供反馈，指导模型调整参数。
- **训练目标**：通过优化算法（如梯度下降）最小化损失。

---

### 1.5 梯度下降（Gradient Descent）
梯度下降是一种优化算法，通过迭代调整模型参数（权重和偏置）来最小化损失。
#### **步骤**：
1. **计算损失**：基于当前参数，计算训练数据的损失。
2. **计算梯度**：通过损失函数对每个参数求偏导，得到梯度，反映损失对参数变化的敏感性。
3. **更新参数**：沿梯度的反方向更新参数，公式为：$w = w - \eta \cdot \frac{\partial L}{\partial w}$，其中$\eta$ 是学习率（learning rate）。

#### **学习率（Learning Rate）**：
- 学习率是参数更新的步长大小，是一个关键超参数。
  - **过小**：收敛速度慢，需要更多迭代。
  - **过大**：可能跳过损失最小值，甚至导致发散。

#### **梯度下降的类型**：
1. **批量梯度下降（Batch Gradient Descent）**：
   - 使用整个数据集计算梯度。
   - 优点：收敛稳定。
   - 缺点：计算成本高，适用于小数据集。
2. **随机梯度下降（Stochastic Gradient Descent, SGD）**：
   - 每次迭代随机选择一个数据点计算梯度。
   - 优点：更新速度快，可能逃离局部极小值。
   - 缺点：更新噪声大，可能不稳定。
3. **小批量梯度下降（Mini-Batch Gradient Descent）**：
   - 使用小批量数据（数据集的一个子集）计算梯度。
   - 优点：平衡了稳定性和计算效率。

---

### 1.6 反向传播（Backpropagation）
反向传播通过链式法则（Chain Rule）将误差从输出层向输入层传播，计算损失对每个参数的梯度。
- **挑战**：
  1. **梯度消失（Vanishing Gradients）**：在深层网络中，梯度可能变得非常小，导致学习缓慢。常见于使用 sigmoid 或 tanh 激活函数。
  2. **梯度爆炸（Exploding Gradients）**：梯度可能变得过大，导致训练不稳定。常见于循环网络。

---

## 第二部分：文本数据处理

### 2.1 文本数据的挑战
文本数据（如推文、新闻、财务报告）具有以下难点：
- **非结构化数据**：文本数据形式多样，包含拼写错误、缩写和噪声。
- **高维性**：词汇量大导致稀疏表示，存在维度灾难，计算效率和内存使用成本高。
- **语义歧义**：同一个词在不同上下文中有不同含义，例如“bank”可能是河岸或金融机构。
- **顺序依赖**：词序影响语义。
- **数值输入需求**：神经网络处理数值而非字符串，文本需映射为数值表示。

---

### 2.2 早期文本表示方法
1. **独热编码（One-Hot Encoding）**：
   - 将每个词表示为唯一向量，维度等于词汇量。
   - 缺点：稀疏且高维，上下文无关，词与词之间的距离无意义。
   - 示例：词汇表为 [Case, Analysis, Economics, and, Finance]，则“Case”表示为 [1, 0, 0, 0, 0]。
2. **词袋模型（Bag-of-Words）**：
   - 基于词频的简单表示，忽略词序和语法。
   - 示例：影评 A: "The movie was not good." 和 B: "The movie was good, not bad." 只记录词频，无法捕捉语义。
3. **TF-IDF（Term Frequency-Inverse Document Frequency）**：
   - 改进词袋模型，不仅考虑词频，还考虑词的稀有度。
   - 优点：突出文档中独特的词，忽略常见词（如“the”）。
   - 缺点：仍然忽略语义和语法关系，生成的矩阵稀疏，计算效率低，无法处理未见词（OOV）。

---

### 2.3 词嵌入（Word Embedding）
#### **定义**：
- 词嵌入是一种现代文本表示方法，将词映射为连续向量空间中的密集向量，编码词的意义、上下文和词间关系。
#### **特点**：
- **语义理解**：捕捉语义和语法信息，语义相近的词在向量空间中距离较近。
- **降维**：将高维稀疏表示（如独热编码）降维为低维密集向量（如300维）。
- **可视化**：通过降维技术（如 t-SNE、PCA）将词嵌入映射到2D空间，展示语义聚类，例如“king”、“queen”等词聚在一起。
- **向量运算**：支持语义运算，例如“king - man + woman ≈ queen”。

#### **词嵌入的演变**：
1. **静态词嵌入**：
   - 每个词有固定向量表示，与上下文无关。
   - 代表方法：Word2Vec、GloVe、FastText。
   - 优点：计算快，可迁移（预训练嵌入可跨任务复用）。
   - 缺点：无法处理一词多义。
2. **上下文词嵌入**：
   - 词向量根据上下文动态变化。
   - 代表方法：BERT、GPT。
   - 优点：更准确捕捉语义。
   - 缺点：计算成本高。

#### **静态词嵌入的创建方法**：
1. **Word2Vec（2013，Google）**：
   - 核心思想：上下文相似的词具有相似含义，嵌入空间中距离较近。
   - 两种模型：
     - **CBOW（Continuous Bag of Words）**：根据上下文预测目标词。
     - **Skip-Gram**：根据目标词预测上下文。
2. **GloVe（Global Vectors）**：
   - 基于语料库中词的全局共现统计。
3. **FastText**：
   - 嵌入子词单元（例如“kingdom”包含“king”和“dom”），更好地处理未见词（OOV）。

---

### 2.4 文本分词（Tokenization）
#### **定义**：
- 分词是将文本分解为更小单元（称为 token）的过程，token 可以是单词、子词或字符。
#### **分词类型**：
1. **单词分词（Word Tokenization）**：
   - 按空格将文本拆分为单词。
   - 示例：`Hello, world! This is an example.` → `['Hello', 'world', 'This', 'is', 'an', 'example']`。
   - 优点：直观，符合人类对单词的理解。
   - 缺点：词汇量大，难以处理未见词（OOV）。
2. **字符分词（Character Tokenization）**：
   - 将文本拆分为单个字符。
   - 示例：`Pikachu` → `['P', 'i', 'k', 'a', 'c', 'h', 'u']`。
   - 优点：词汇量小，无 OOV 问题，语言无关。
   - 缺点：失去语义，序列长，计算复杂。
3. **子词分词（Subword Tokenization）**：
   - 将单词拆分为更小的子词单元，使用方法如字节对编码（BPE）、WordPiece。
   - 优点：结合单词和字符分词的优势，减少词汇量，处理 OOV。
   - 缺点：预处理复杂，子词边界可能不符合直觉。

#### **字节对编码（BPE）**：
- **定义**：BPE 是一种子词分词技术，通过迭代合并频繁出现的字符对生成子词。
- **工作步骤**：
  1. 初始化：将语料库中每个词拆分为字符，并添加词尾标记（如“w” → “w_”）。
  2. 统计字符对频率：计算相邻字符对出现的频率。
  3. 合并最频繁字符对：将最频繁的字符对合并为一个单元。
  4. 迭代重复：继续合并直到达到预定义的合并次数或词汇量。
  5. 编码未见词：对于新词，使用已知子词进行拆分。
- **示例**：
  - 语料库：“low low low low low lower lower newest newest newest newest newest newest widest widest widest”
  - 初始化：`(l, o, w, _): 5, (l, o, w, e, r, _): 2, (n, e, w, e, s, t, _): 6, (w, i, d, e, s, t, _): 3`
  - 合并 1：合并`(e, s)` → `es`（频率9次）。
  - 合并 2：合并`(es, t)` → `est`（频率9次）。
  - 合并 3：合并`(est, _)` → `est_`（频率9次）。
  - 合并 4：合并`(l, o)` → `lo`（频率7次）。
  - 合并 5：合并`(lo, w)` → `low`（频率7次）。
  - 最终词汇表：`l, o, w, e, r, n, s, t, i, d, _, es, est, est_, lo, low`
  - 对新文本“newest bindedlowers”分词：
    - 初始：`(n, e, w, e, s, t, _), (b, i, n, d, e, d, _), (l, o, w, e, r, s, _)`
    - 应用合并规则：`(n, e, w, est_), ([UNK], i, n, d, e, d, _), (low, e, r, s, _)`
    - 结果：`[n, e, w, est_, [UNK], i, n, d, e, d, _, low, e, r, s, _]`

#### **将 token 转换为 token ID**：
- 神经网络只能处理数值表示，因此需要将 token 转换为整数形式的 token ID。
- 步骤：
  1. 构建词汇表：建立 token 和唯一 ID 的映射。
  2. 编码：将 token 序列转换为 token ID 序列。
  3. 解码：将 token ID 转换回 token，用于解释或调试。

---

## 第三部分：循环神经网络（RNN）

### 3.1 序列数据的重要性
- **序列数据示例**：时间序列（如股票价格）、文本（如句子中的单词）、音频、传感器数据。
- **时间依赖性**：某一时刻的事件可能影响未来时刻的结果。
- **前馈神经网络 vs. RNN**：
  - 前馈神经网络忽略数据顺序。
  - RNN 显式处理时间或序列依赖性。

---

### 3.2 什么是 RNN？
- RNN（Recurrent Neural Network）是一种具有循环连接的神经网络，适合处理序列数据。
- **核心特性**：
  - 通过隐藏状态$h$保留先前输入的信息。
  - 在每个时间步$t$，网络接收当前输入$x_t$和上一时间步的隐藏状态$h_{t-1}$，生成新的隐藏状态$h_t$。
- **参数共享**：在每个时间步使用相同的权重矩阵，减少模型参数，确保对相同输入的一致性处理。

#### **公式**：
- 隐藏状态更新：$h_t = f(W_h h_{t-1} + W_x x_t + b_h)$，其中$f$为激活函数。
- 输出计算：$o_t = W_o h_t + b_o$。

#### **RNN 的类型**：
- **多对一（Many-to-One）**：如语言模型，输入文本序列，输出下一个单词。
- **多对多（Many-to-Many）**：如机器翻译，输入一种语言的文本序列，输出另一种语言的文本序列。

---

### 3.3 应用示例
1. 语音识别：波形序列 → 文本序列。
2. 机器翻译：一种语言文本序列 → 另一种语言文本序列。
3. 语言模型：文本序列 → 下一个单词。
4. 股票预测：市场数据序列 → 下一天/年价格或方向。

---

### 3.4 通过时间的反向传播（Backpropagation Through Time, BPTT）
- BPTT 是 RNN 的训练方法，通过时间步展开网络，将误差反向传播，计算梯度。
- **挑战**：
  1. **梯度消失（Vanishing Gradients）**：梯度随时间步长指数衰减，难以学习长距离依赖。
  2. **梯度爆炸（Exploding Gradients）**：梯度随时间步长指数增长，导致训练不稳定。
- **原因**：多时间步重复使用权重矩阵，若权重值大于1或小于1，会导致梯度累积放大或缩小。

---

### 3.5 长短期记忆网络（LSTM）
- **设计目的**：解决 RNN 的梯度消失问题，通过门机制控制信息的流动。
- **门机制**：
  1. **遗忘门（Forget Gate）**：决定保留或丢弃多少旧信息，输入为$h_{t-1}$和$x_t$，输出为0到1的值。
  2. **输入门（Input Gate）**：决定添加多少新信息到单元状态，结合 sigmoid 和 tanh 函数调节信息重要性。
  3. **输出门（Output Gate）**：决定下一个隐藏状态的内容，基于更新后的单元状态和 sigmoid 输出。
- **优点**：能捕捉长距离依赖，梯度可以通过单元状态“高速公路”传递。

---

### 3.6 门控循环单元（GRU）
- GRU 是 LSTM 的简化版，使用两个门（更新门和重置门），参数更少，训练更快。
- **优点**：在许多数据集中性能接近 LSTM，计算成本低。
- **缺点**：在极长依赖任务中可能不如 LSTM 有效。

---

### 3.7 LSTM、GRU 和 Vanilla RNN 的对比
1. **Vanilla RNN**：
   - 优点：结构简单，参数少，计算快。
   - 缺点：易受梯度消失和爆炸影响，难以处理长依赖。
   - 适用场景：短序列任务，教育或概念验证。
2. **LSTM**：
   - 优点：通过门机制克服梯度消失，适合长距离依赖任务。
   - 缺点：参数多，训练稍慢。
   - 适用场景：语言建模、机器翻译、长时间序列预测。
3. **GRU**：
   - 优点：比 LSTM 简单，训练快，参数少。
   - 缺点：长依赖捕捉能力略逊于 LSTM。
   - 适用场景：时间序列、语音和 NLP 任务，需平衡性能和计算成本。

---

### 3.8 双向循环神经网络（Bi-RNN）
- **定义**：Bi-RNN 同时从左到右（前向 RNN）和从右到左（反向 RNN）处理输入序列，结合两方向的输出。
- **优点**：
  - 增强上下文理解：考虑过去和未来的信息。
  - 提高精度：适用于需要未来上下文的任务，如语音识别、文本标注。
- **应用**：
  - NLP：词性标注、命名实体识别。
  - 语音处理：利用整个音频片段的上下文。

---

## 总结
本讲内容涵盖了神经网络基础（神经元、梯度下降、反向传播）、文本数据处理（词嵌入、分词）和循环神经网络（RNN、LSTM、GRU、Bi-RNN）的核心概念和技术。通过补充背景知识和示例解答，希望这份文档能帮助你全面理解课程内容。如有进一步问题，欢迎随时提问！